[{"title":"kubernetes中YAML文件学习","date":"2018-12-11T12:54:54.000Z","path":"2018/12/11/kubernetes中YAML文件学习/","text":"在kubernetes中我们有两种方式可以去创建资源，第一是使用kubernetes run命令来直接运行，还有一种就是使用YAML或JSON格式的配置文件的方式。YAML配置文件就像kvm里虚拟机的XML文件一样记录了你创建资源的详细信息，可以像管理代码一样来管理配置文件，适合正式的具有规模的生产环境。下面看一个nginx应用的配置文件: nginx-deployment.yaml12345678910111213141516apiVersion: app/v1kind: Deploymentmetadata: name: nginx-deploymentspec: replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports： - containerPort: 80 ==apiVersion== 是当前配置格式的版本==kind== 是要创建的资源类型，这里是Deployment==metadata== 是该资源的元数据，name是必须的元数据项==spec== 部分是该Deployment的规格说明==replicas== 指明副本数量，默认为1==template== 定义pod的模板，这是配置而文件的重要部分==metadata== 定义pod的元数据，至少要定义一个label。Label的key和 value可以任意指定,controller可以通过label从kubernetes中过滤出它所关心的被控制对象。==spec== 描述pod的规格，此部分定义pod中每一个容器的属性,name和image是必须的。 在一个YAML文件（API对象）中，大部分可以分为metadata和spec两部分，前者存放元数据，对于所有的文件来说，这一部分基本都是一样的；后者存放的属于这个对象独有的，用来描述它要表达的功能。12345几个命令Kubectl apply –f nginx-deployment.yaml (创建、更新等都使用apply)Kubectl describe pod nginx-deployment的nameKubectl exec –it pod的name -- /bin/bashKubectl delete –f nginx-deployment.yaml 另外，kubernetes中，volume也是属于pod对象的一部分，我们可以在这个yaml文件中添加一个volume。如下，修改ymal文件：12345678910111213141516171819202122apiVersion: app/v1kind: Deploymentmetadata: name: nginx-deploymentspec: replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports： - containerPort: 80 volumeMounts: - mountPath: ‘/usr/share/nginx/html’ name: nginx-vol volumes: - name: nginx-vol emptyDir: &#123;&#125; 我们在Deployment的pod模板部分添加了一个volume字段。定义了这个pod申明的所有volume,名字是nginx-vol,类型是emptyDir。emptyDir类型是隐式volume参数，即不显式声明宿主机目录的volume，kubernetes会在宿主机上创建一个临时目录，这个目录会被挂载到容器所声明的目录上，volumeMounts声明自己挂载哪个volume，mountPath定义容器内的volume目录。当pod在node上移除时，emptyDir中的数据也会被永久删除。当然,kubernetes也提供显式的volume定义，叫做hostPath。例如：12345… volumes: - name: nginx-vol hostPath: path: /var/data 这样，容器volume挂载的宿主机目录，就是/var/data 当然，kubernetes提供的volume类型还有很多，比如：gcePersistentDisk(谷歌公有云的永久磁盘)、awsElasticBlockStore(亚马逊公有云提供的volume)、NFS、iscsi、glusterfs、rbd等等。","categories":[{"name":"技术","slug":"技术","permalink":"http://www.chengshaojin.com/categories/技术/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://www.chengshaojin.com/tags/kubernetes/"}]},{"title":"使用kubeadm安装kubernetes v1.12","date":"2018-11-02T12:26:28.000Z","path":"2018/11/02/使用kubeadm安装kubernetes-v1-12/","text":"目前大部分的kubernetes生产环境都是使用SaltStack、Ansible等运维工具自动化安装kebernetes的二进制文件，本身这些专业运维工具学习就比较费劲，而且用户还要自己编写配置文件、授权文件等，比较麻烦。有麻烦就有解决办法，kubeadm就是解决这些麻烦的，旨在通过kubeadm init和kubeadn join这两条指令一键部署kubernetes。但是截至目前，kubeadm还没有解决一键部署一个高可用的kubernetes集群问题。但是，kubeadm已经GA 版本了，离成功也已经不远了，而且使用kubeadm对理解kubernetes组件的工作方式和架构是非常有帮助的。接下来我们就使用kubeadm全程用阿里云(不需要翻墙)来部署kubernetes集群。。 一、准备环境1.操作系统12[root@k8s-1 ~]# cat /etc/redhat-release CentOS Linux release 7.2.1511 (Core) 所有主机操作系统相同，均为最小化安装 2.docker版本12[root@k8s-1 ~]# docker -vDocker version 18.06.0-ce, build 0ffa825 kubernetes 1.12最低支持的docker版本是1.11.13.kubernetes版本12[root@k8s-1 ~]# kubeadm versionkubeadm version: &amp;version.Info&#123;Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.2\", GitCommit:\"17c77c7898218073f14c8d573582e8d2313dc740\", GitTreeState:\"clean\", BuildDate:\"2018-10-24T06:51:33Z\", GoVersion:\"go1.10.4\", Compiler:\"gc\", Platform:\"linux/amd64\"&#125; 4.各主机主机名及IP配置1234567[root@k8s-1 ~]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.56.128 k8s-1192.168.56.129 k8s-2192.168.56.130 k8s-3 本次实验一共3台主机，一台用于master部署，其他为node节点。 二、架构图 三、安装前操作每台主机都必须执行的步骤1.关闭并禁用防火墙12[root@k8s-1 ~]# systemctl stop firewalld[root@k8s-1 ~]# systemctl disable firewalld 2.关闭selinux12[root@k8s-1 ~]# sed -i 's/enforcing/disabled/' /etc/selinux/config[root@k8s-1 ~]# setenforce 0 3.禁用swap12[root@k8s-1 ~]# swapoff -a永久禁用的话修改/etc/fstab，注释swap，建议永久禁用，因为重启操作系统的话如果swap没禁用kubelet服务起不来。 4.修改/etc/hosts文件5.开启ipv4转发，启用bridge-nf-call-iptables和bridge-nf-call-ip6tables1234[root@k8s-1 ~]# echo \"net.ipv4.ip_forward = 1\"&gt;&gt;/etc/sysctl.conf[root@k8s-1 ~]# echo 'net.bridge.bridge-nf-call-iptables = 1'&gt;&gt;/etc/sysctl.conf[root@k8s-1 ~]# echo 'net.bridge.bridge-nf-call-ip6tables = 1'&gt;&gt;/etc/sysctl.conf[root@k8s-1 ~]# sysctl -p 6.配置docker的yum源，并安装docker-ce-18.06.012345678[root@k8s-1 ~]# echo '[docker-ce-stable] name=Docker CE Stable - $basearch baseurl=https://download.docker.com/linux/centos/7/$basearch/stable enabled=1 gpgcheck=1 gpgkey=https://download.docker.com/linux/centos/gpg'&gt;/etc/yum.repos.d/docker-ce.repo[root@k8s-1 ~]# yum -y install docker-ce-18.06.0.ce-3.el7[root@k8s-1 ~]# systemctl start docker &amp;&amp; systemctl enable docker 7.配置kubernetes阿里云镜像源1234567[root@k8s-1 ~]# echo '[kubernetes]&gt; name=Kubernetes&gt; baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64&gt; enabled=1&gt; gpgcheck=1&gt; repo_gpgcheck=1&gt; gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg'&gt;/etc/yum.repos.d/kubernetes.repo 四、使用kubeadm部署kubernetes1.在每个节点安装kubeadm、kubelet、kubectl12[root@k8s-1 ~]# yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes[root@k8s-1 ~]# systemctl enable kubelet &amp;&amp; systemctl start kubelet 2.以下在master节点k8s-1上执行12345678910111213141516171819202122[root@k8s-1 ~]# kubeadm init --kubernetes-version=1.12.1 --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=192.168.56.128[init] using Kubernetes version: v1.12.1[preflight] running pre-flight checks[preflight/images] Pulling images required for setting up a Kubernetes cluster[preflight/images] This might take a minute or two, depending on the speed of your internet connection[preflight/images] You can also perform this action in beforehand using 'kubeadm config images pull'[preflight] Some fatal errors occurred: [ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-apiserver:v1.12.1: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers), error: exit status 1 [ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-controller-manager:v1.12.1: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers), error: exit status 1 [ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-scheduler:v1.12.1: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers), error: exit status 1 [ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-proxy:v1.12.1: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers), error: exit status 1 [ERROR ImagePull]: failed to pull image k8s.gcr.io/pause:3.1: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers), error: exit status 1 [ERROR ImagePull]: failed to pull image k8s.gcr.io/etcd:3.2.24: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers), error: exit status 1 [ERROR ImagePull]: failed to pull image k8s.gcr.io/coredns:1.2.2: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers), error: exit status 1[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...` 报以上错误，就是因为不能翻墙。。。根据报错信息，在阿里云上下载相关镜像，然后修改标签。123456789101112131415# docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver-amd64:v1.12.1# docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager-amd64:v1.12.1# docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler-amd64:v1.12.1# docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy-amd64:v1.12.1# docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:3.2.24# docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1# docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.2.2重新打tag# docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 k8s.gcr.io/pause:3.1# docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.2.2 k8s.gcr.io/coredns:1.2.2# docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:3.2.24 k8s.gcr.io/etcd:3.2.24# docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler-amd64:v1.12.1 k8s.gcr.io/kube-scheduler:v1.12.1# docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager-amd64:v1.12.1 k8s.gcr.io/kube-controller-manager:v1.12.1# docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver-amd64:v1.12.1 k8s.gcr.io/kube-apiserver:v1.12.1# docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy-amd64:v1.12.1 k8s.gcr.io/kube-proxy:v1.12.1 3.编辑以下配置文件/var/lib/kubelet/kubeadm-flags.env1[root@k8s-1 ~]# echo 'KUBELET_KUBEADM_ARGS=--cgroup-driver=cgroupfs --cni-bin-dir=/opt/cni/bin --cni-conf-dir=/etc/cni/net.d --network-plugin=cni'&gt;/var/lib/kubelet/kubeadm-flags.env 本步骤主要是核实cgroup-driver=cgroupfs4.重新初始化master1[root@k8s-1 ~]# kubeadm init --kubernetes-version=1.12.1 --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=192.168.56.128 初始化成功以后，根据提示执行以下步骤12345 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config将其他两个节点加入集群，分别在两个节点执行以下命令： kubeadm join 192.168.56.128:6443 --token 9azy5c.kpl624h4x73msiqz --discovery-token-ca-cert-hash sha256:b78daff96d6e4e59786df7c0431082799d41c66f61ac1f26caf3777c88ef7d60 在注册集群的时候出现以下告警，是因为最小化安装操作系统没有包含该模块，对于实验和简单使用没有影响1234[WARNING RequiredIPVSKernelModulesAvailable]: the IPVS proxier will not be used, because the following required kernel modules are not loaded: [ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh] or no builtin kernel ipvs support: map[ip_vs:&#123;&#125; ip_vs_rr:&#123;&#125; ip_vs_wrr:&#123;&#125; ip_vs_sh:&#123;&#125; nf_conntrack_ipv4:&#123;&#125;]you can solve this problem with following methods:1. Run 'modprobe -- ' to load missing kernel modules;2. Provide the missing builtin kernel ipvs support 5.安装flanel1234567891011121314151617181920212223242526272829303132333435363738在master节点执行：[root@k8s-1 ~]# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml执行以下命令：# kubectl get pod --all-namespaces看到有的pod不是running状态，使用describe查看发现其他节点还是因为被墙pull不下来镜像，还得去阿里云下载并改tag# kubectl describe pod kube-flannel-ds-amd64-bbr75 --namespace=kube-system在其他两个节点执行以下命令：# docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1# docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 k8s.gcr.io/pause:3.1# docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy-amd64:v1.12.1# docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy-amd64:v1.12.1 k8s.gcr.io/kube-proxy:v1.12.1# docker pull registry.cn-hangzhou.aliyuncs.com/kubernetes_containers/flannel:v0.10.0-amd64# docker tag registry.cn-hangzhou.aliyuncs.com/kubernetes_containers/flannel:v0.10.0-amd64 quay.io/coreos/flannel:v0.10.0-amd64再去看下，都是running[root@k8s-1 ~]# kubectl get pods --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system coredns-576cbf47c7-7wfhr 1/1 Running 7 3dkube-system coredns-576cbf47c7-qdbzv 1/1 Running 7 3dkube-system etcd-k8s-1 1/1 Running 2 3dkube-system kube-apiserver-k8s-1 1/1 Running 2 3dkube-system kube-controller-manager-k8s-1 1/1 Running 2 3dkube-system kube-flannel-ds-amd64-bbr75 1/1 Running 3 3dkube-system kube-flannel-ds-amd64-gq8v5 1/1 Running 2 3dkube-system kube-flannel-ds-amd64-qd8cj 1/1 Running 3 3dkube-system kube-proxy-d6cjg 1/1 Running 2 3dkube-system kube-proxy-ghkmk 1/1 Running 2 3dkube-system kube-proxy-rvr4j 1/1 Running 2 3dkube-system kube-scheduler-k8s-1 1/1 Running 2 3d[root@k8s-1 ~]# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-1 Ready edge,master 3d v1.12.2k8s-2 Ready &lt;none&gt; 3d v1.12.2k8s-3 Ready &lt;none&gt; 3d v1.12.2[root@k8s-1 ~]# kubectl get csNAME STATUS MESSAGE ERRORscheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy &#123;\"health\": \"true\"&#125; 6.安装dashboard在master节点执行12# cd /etc/kubernetes# wget https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml 编辑kubernetes-dashboard.yaml文件,修改镜像下载地址：12- name: kubernetes-dashboard image: registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:v1.10.0 master节点执行以下：1# kubectl create -f kubernetes-dashboard.yaml 查看pod dashboard是否正常，不正常的话describe看下，是不是没有pull下来镜像，如果没有pull下来再手动去提示的节点pull dashboard的镜像，pull下来以后再去看，就会正常。7.授予Dashboard账户集群管理权限需要一个管理集群admin的权限，新建kubernetes-dashboard-admin.rbac.yaml文件，内容如下123456789101112131415161718192021222324[root@k8s-1 kubernetes]# cat kubernetes-dashboard-admin.rbac.yaml apiVersion: v1kind: ServiceAccountmetadata: name: admin-user namespace: kube-system---# Create ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: admin-userroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: admin-user namespace: kube-system执行命令：# kubectl create -f kubernetes-dashboard-admin.rbac.yaml 找到kubernete-dashboard-admin的token，用户登录使用12# kubectl -n kube-system get secret | grep admin-user# kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '&#123;print $1&#125;') 这个token就是登陆用的，默认永久有效8.dashboard的三种访问方式第一.kubectl proxy方式访问master上执行以下命令：1# kubectl proxy --address=0.0.0.0 --disable-filter=true 即可通过浏览器访问: http://192.168.56.128:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login1注意：仪表盘使用kubectl代理命令不应暴露公开,因为它只允许HTTP连接。域以外的localhost和127.0.0.1将不能登录。在登录页面点击登录按钮什么都不会发生后，跳过登录后，没有任何权限。 第二.NodePort方式访问这种访问方式仪表板只建议在单个节点上设置开发环境。 编辑kubernetes-dashboard.yaml文件，添加type: NodePort和nodePort: 30001，暴露Dashboard服务为30001端口,参考如下；12345678910111213141516# ------------------- Dashboard Service ------------------- #kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: type: NodePort # NodePort登录方式 ports: - port: 443 targetPort: 8443 nodePort: 30001 # NodePort登录暴露端口 selector: k8s-app: kubernetes-dashboard 使用节点ip和port访问 https://:第三.API Server方式访问(推荐访问方式)（好像只能用谷歌内核的浏览器）使用以下访问 https://:/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/例如：https://192.168.56.128:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/浏览器访问问题：1234567891011121314 &quot;kind&quot;: &quot;Status&quot;, &quot;apiVersion&quot;: &quot;v1&quot;, &quot;metadata&quot;: &#123; &#125;, &quot;status&quot;: &quot;Failure&quot;, &quot;message&quot;: &quot;services \\&quot;https:kubernetes-dashboard:\\&quot; is forbidden: User \\&quot;system:anonymous\\&quot; cannot get services/proxy in the namespace \\&quot;kube-system\\&quot;&quot;, &quot;reason&quot;: &quot;Forbidden&quot;, &quot;details&quot;: &#123; &quot;name&quot;: &quot;https:kubernetes-dashboard:&quot;, &quot;kind&quot;: &quot;services&quot; &#125;, &quot;code&quot;: 403&#125; 这是因为最新版的k8s默认启用了RBAC，并为未认证用户赋予了一个默认的身份：anonymous,对于API Server来说，它是使用证书进行认证的我们需要先创建一个证书： 首先找到kubectl命令的配置文件，默认情况下为/etc/kubernetes/admin.conf，在 上文 中，我们已经复制到了$HOME/.kube/config中。 然后我们使用client-certificate-data和client-key-data生成一个p12文件，可使用下列命令：12345678# 生成client-certificate-datagrep 'client-certificate-data' ~/.kube/config | head -n 1 | awk '&#123;print $2&#125;' | base64 -d &gt;&gt; kubecfg.crt# 生成client-key-datagrep 'client-key-data' ~/.kube/config | head -n 1 | awk '&#123;print $2&#125;' | base64 -d &gt;&gt; kubecfg.key# 生成p12openssl pkcs12 -export -clcerts -inkey kubecfg.key -in kubecfg.crt -out kubecfg.p12 -name \"kubernetes-client\" 最后在浏览器里导入上面生成的p12文件，重新打开浏览器，显示出现选择证书选项，选OK，然后就可以看到熟悉的登录界面了。我们可以使用一开始创建的admin-user用户的token进行登录，一切OK。终于大功告成了！","categories":[{"name":"技术","slug":"技术","permalink":"http://www.chengshaojin.com/categories/技术/"}],"tags":[{"name":"kubetnetes","slug":"kubetnetes","permalink":"http://www.chengshaojin.com/tags/kubetnetes/"}]},{"title":"两个神奇的命令","date":"2018-07-05T07:54:47.000Z","path":"2018/07/05/两个神奇的命令/","text":"一、script&emsp;&emsp;script命令可以把你在当前终端上的所有操作包括输出到屏幕上的内容都保存到指定的文件中，然后可以通过文本编辑器打开查看。也可以用script将终端的操作录制下来，用scriptreplay将录制的内容播放出来。简直就是一个神器啊，早点知道这个命令，很多个跑脚本或ansible的时候就可以去吃个饭或者睡个觉，回来再看过程及结果，多美好的一件事啊。 看看script的options:123456789101112131415[root@queens ~]# script --helpUsage: script [options] [file]Options: -a, --append append the output 在已经有的输出文件里追加新的内容 -c, --command &lt;command&gt; run command rather than interactive shell 直接将后面跟的命令输出结果保存到文件里 -e, --return return exit code of the child process 子进程中返回退出代码 -f, --flush run flush after each write 如果需要在输出到日志文件的同时，也可以查看日志文件的内容 --force use output file even when it is a link -q, --quiet be quiet 以静默模式运行 -t, --timing[=&lt;file&gt;] output timing data to stderr (or to FILE) 指明输出录制的时间数据 -V, --version output version information and exit -h, --help display this help and exit 退出script按ctrl+d或者输入exit123456789101112131415161718# script test.his 将终端操作保存在test.his文件中[root@queens ~]# script test.hisScript started, file is test.his[root@queens ~]# cd /etc/sysconfig/networkbash: cd: /etc/sysconfig/network: Not a directory[root@queens ~]# cd /etc/sysconfig/network-scripts/[root@queens network-scripts]# exitexitScript done, file is test.his[root@queens ~]# cat test.his Script started on Thu 05 Jul 2018 04:53:06 PM CST[root@queens ~]# cd /etc/sysconfig/networkbash: cd: /etc/sysconfig/network: Not a directory[root@queens ~]# cd /etc/sysconfig/network-scripts/[root@queens network-scripts]# exitexitScript done on Thu 05 Jul 2018 04:53:24 PM CST 123456789101112录制终端[root@queens ~]# script -t 2&gt;test.time -a test.hisScript started, file is test.his# 开始录制了[root@queens ~]# ls[root@queens ~]# exitexitScript done, file is test.his# 结束录制# 播放录制：[root@queens ~]# scriptreplay test.time test.hisScript started, file is test.time 说明：如果只是把终端操作保存在文件里不录制的话，你编辑文件的操作不会被保存，只把你打开文件的命令会保存。录制的话是所有操作完整保存播放，相当于录屏。 二、Tmux&emsp;&emsp;tmux是”Terminal Multiplexer”的简称，是一款终端复用的软件，用于以下场景：1.希望关闭终端,再次打开时原终端里面的任务进程依然不会中断 ;2.处于异地的两人可以对同一会话进行操作，一方的操作另一方可以实时看到 ;3.可以在单个屏幕的灵活布局下开出很多终端，也就是分屏，然后就能协作地使用它们 ;123456789101112131.安装tmux# yum install tmux2.新建会话# tmux new -s cheng (cheng为新建的会话名)新建就直接进入新的会话了，在会话里不能再新建会话。3.暂时离开会话# tmux detach4.重新进入会话# tmux a -t cheng 或 # tmux attach -t cheng5.查看已创建的会话# tmux ls6.关闭会话# tmux kill-session -t cheng 分屏操作123456789101112131415进入会话后可以将终端屏幕分屏1.水平分屏：快捷键：先按 ctrl+b, 放开后再按% 2.垂直分屏:快捷键：先按 ctrl+b, 放开后再按 \"3.分屏后的窗口中的光标互相切换:快捷键：先按ctrl+b, 放开后再按下o4.切换tmux会话终端:快捷键：先按ctrl+b, 放开后再按s5.终端内显示时间:快捷键：先按ctrl+b, 放开后再按t ,出时间界面：按q键6.终止一个终端窗口(需确认) 快捷键：exit 或 先按ctrl+b, 放开后再按 &amp; 7.暂时退出当前会话:快捷键：先按ctrl+b, 放开后再按 d","categories":[{"name":"技术","slug":"技术","permalink":"http://www.chengshaojin.com/categories/技术/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://www.chengshaojin.com/tags/Linux/"}]},{"title":"2018年俄罗斯世界杯","date":"2018-06-13T13:59:22.000Z","path":"2018/06/13/2018年俄罗斯世界杯/","text":"&emsp;&emsp;明晚就是2018年俄罗斯世界杯开幕式了，时间真的是手中的沙，悄悄的无时无刻的在溜走，我还能清楚的记得四年前跟球队队友在酒吧里看巴西世界杯的情形：揭幕战是东道主巴西对克罗地亚，马塞洛奉献了当年世界杯的第一粒进球，而且是踢进了自家大门，中场结束的时候是1：1，因为喝太多啤酒，加上凌晨4点多的球赛我在中场结束便睡倒在沙发上。一觉醒来的结果是巴西逆转3-1克罗地亚，巴西头号球星内马尔梅开二度，似乎当年巴西捧杯是很有希望的样子，谁也想不到在半决赛5星巴西被德国以7-1的成绩横扫。我也清楚的记得那年我们校队的成绩也是非常的不堪，大家还相约一定要好好训练，来年大学生足球联赛一定要复仇。这些上届世界杯的事情还历历在目，转眼就又是一年新的世界杯。 这是四年前在昏暗的酒吧用渣渣像素的手机拍的队友，醉眼迷离的样子，哈哈。&emsp;&emsp;白岩松说：俄罗斯世界杯，中国除了足球队没去，基本上其他都去了。有数据说，中国球迷买的世界杯门票在所有国家中排名第九，17个赞助商中有5家中国企业，世界杯吉祥物全都由中国制造，什么看台电梯啊，小龙虾啊之类的都去了俄罗斯。听说本次世界杯，只要你手里有世界杯门票就可以免签证，这个对球迷来说倒是方便了不少。&emsp;&emsp;有人总是预测世界杯的冠军，今年表面实力最强的就是法国，德国，巴西，西班牙这四支队伍了，但是每年世界杯总有惊奇与失望，西班牙卫冕冠军上届照样踢不出小组赛，意大利荷兰今年连俄罗斯都没去成，全国人口30多万人的冰岛队也能进决赛圈。所以，我觉得世界杯球队跟联赛球队不一样，实力是一部分，还有很大一部分要看球队的士气、团结度、荣誉感、运气等等，所以拥有一位球队领袖并且是一位超级球星的阿根廷队不容小觑。还有比利时也是我非常看好的一支球队，而且很多事情是赢到最后的都不是最初实力最强的。&emsp;&emsp;除了几大热门球队，本次世界杯还有一下看点,大家应该去关注：&emsp;&emsp;1.冰岛和巴拿马都是首次杀入世界杯，并且冰岛也是世界杯历史上人口最少的参赛国，全国居民仅有33.4万人，期待他们的表现。&emsp;&emsp;2.以前的世界杯冠军都是欧洲或者南美洲的球队。希望亚洲和非洲球队能突破重围，有好的表现。&emsp;&emsp;3.2014年卫冕冠军西班牙与2010年卫冕冠军意大利以及06年的法国都没能踢出小组赛，看看德国是否能打破魔咒，或者一举再夺大力神杯。&emsp;&emsp;4.现役球员中，托马斯·穆勒的世界杯进球是最多的，共有10球，相比于世界杯史上进球最多的克洛泽，他还差6球（16球）。&emsp;&emsp;5.巴西能否为上届家门口的七剑之仇复仇？那场球赛对巴西人来说就是一场噩梦。&emsp;&emsp;6.比利时可能会是本次世界杯的黑马，而且他们的进球绝大部分都是在下半场。&emsp;&emsp;7.东道主俄罗斯从未出过小组线，看看这次在家门口是否能出线。&emsp;&emsp;8.会有哪些新的球星闪耀，很值得我们期待。&emsp;&emsp;9.（妹子福利）本次世界杯最值得看的肉体：罗伊斯（德国）、迪巴拉（阿根廷“陈冠希”）、格里兹曼（法国）、哈梅斯·罗德里格斯（哥伦比亚）、杰拉德·皮克（西班牙）、凯文·特拉普（德国）等。照片自行百度。最后还是期待中国队第二次进入世界杯决赛圈。&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;—-2018/6/13 &emsp;&emsp;世界杯的小组赛踢完了，德国还是倒在了小组赛里，世界杯卫冕冠军踢不出小组赛的魔咒依然存在。只要德国1-0战胜韩国就能出线，可惜韩国并没有给德国机会，反而被韩国抓住机会进了两个球，锁定了韩国的胜局。看台上的克洛泽看着这场糟糕的比赛，很多还是上届世界杯他的队友，眼里全是落寞与失望。与其说韩国防守的顽强，不如说德国将场上优势转化为进球的效率太低，很早大家都能看出来的问题，可是就是没有办法解决。有人在六个月前就预测了F组的出线形势，不得不说这位看的还是比较透。https://www.zhihu.com/question/263456403/answer/269355419 还是很喜欢德国这支球队的，他们底蕴深厚，人才储备丰富，相信他们能很快调整走出低谷，这才是强队与弱队之间的区别。可是对于本次世界杯的一些球员来说，本次世界杯可能就是他们最后一届世界杯了，时间是不会等待他们的，我们常常说的还有下次却是最好的机会，可是对于很多球员来说我们可能再也不会在世界杯的赛场上看见他们了。 &emsp;&emsp;小组赛期间让人开心的就是大学球队的聚会了，毕业两年回到大学四年踢球的那片操场，真是让人怀念的地方，操场旁边的小卖部也还在，负责经营的张老师的儿子也已经长大了，也开始在操场追着足球跑了。聚会的那天晚上是阿根廷和冰岛的比赛，最后结果大家也知道了，梅西罚失点球，冰岛逼平了阿根廷，一瞬间很多人都开始怪罪于梅西。后来阿根廷0-3克罗地亚，好在有惊无险的战胜了尼日利亚顺利出线，梅西也用进球有力的反击那些质疑的人。 &emsp;&emsp;很多人都传世界杯的假球，但我依然不相信这些奔跑在球场为荣誉而战的年轻人们会被金钱所收买，只是大家不想承认自己心里的豪强被所谓的弱队所击败。很多在大多数人眼中都不具备小组出线的绝对实力的球队，他们却都正在无限接近那个近乎不可能完成的任务。正是这些球队，让世界杯的比赛不再强弱分明，让任何一支传统豪门都难以再有一骑绝尘的底气。当卫冕冠军在完成绝杀感谢上天救赎的时候，当内马尔在小组赛中就已经泪流满面的时候，世界杯已经被呈现的更加精彩，没有谁会成为永远的主角。超越自己，超越对手，才是正赛场上永恒不变的主题。&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;—-2018/6/30","categories":[{"name":"生活","slug":"生活","permalink":"http://www.chengshaojin.com/categories/生活/"}],"tags":[{"name":"世界杯","slug":"世界杯","permalink":"http://www.chengshaojin.com/tags/世界杯/"}]},{"title":"读苏轼与杂谈","date":"2018-05-27T10:07:55.000Z","path":"2018/05/27/读苏轼与杂谈/","text":"&emsp;&emsp;感觉苏东坡要火，这位接近一千年前的大文豪，不仅影响着当初的时政与生活，而且在之后的一千年，他的诗词，他对生活的乐观态度，依然被很多的人推崇。就是历史上有这样一种人，不管他是身居高位还是穷困潦倒，在他们的内心里总是感到满足和富有，这种内心的满足是那种宠辱不惊，是那种豪放不羁，是那种遗世独立，不管在政治和生活上怎么压迫他，在别人看来仿佛他都会过的很好，让人只能望其项背，无法企及。其实事实也是如此。苏东坡就是这样的一种人。 &emsp;&emsp;一0八0年的旧年除夕，因为乌台诗案而被捕的苏轼，在狱中度过四个月又二十天后被释出狱。这天他走出监狱大门，闻到外面清新自由的空气，微风吹在脸颊上，他看见行人骑马走在街上。回到家，他便做了诗，他说：”却对酒杯浑似梦，试拈诗笔已如神”,他觉得自己诗如泉涌，又说：”平生文字为吾累，此去声名不厌低。塞上纵归他日马，城东不斗少年鸡”。这位可爱的诗人刚出狱就做这样的诗，如果那些御史仔细检查他的诗，两首诗又能当作他诽谤和对帝王不敬的证据。因为他日马可以理解为塞翁失马，不知是好运还是厄运，但是少年鸡则指贾昌年轻时因斗鸡而受唐天子恩宠，在宫内瞎折腾，可引申为朝廷当政的小人，这不就是诽谤么？他自己做完诗都笑道，”我真是不可救药”。我们可以看出苏轼的心比天大，他天生乐观豁达，我们都从小知道他是北宋著名诗人，词人，但是苏轼的一生并没有把所有精力致力于写作，读完林语堂先生的《苏东坡传》，我们可以知道，他还是一个悲天悯人的道德家、一个新派画家、一个瑜伽术的修炼者、是佛教徒、是士大夫、是法官是书法家、是皇帝秘书是散文作家、还是一个饮酒成癖者。他的乐天与他广泛的兴趣是分不开的，我们可以想一想，如果苏轼除了诗词当官以外，对其他的事物没有多大的好奇心，那么在他被贬的岁月里，除了每天作诗评论当政者的愚昧感叹命运的不公，他还能做些什么呢？他又如何乐观的起来呢？所幸，苏轼是一位拿的起放的下的人。在其政，他最高官居副宰相，但他坚持为官一地，造福一方，徐州防汛，杭州筑堤，儋州授馆，兴修水利，架桥凿井，赈灾施药；不在其政，他虽心系百姓，但依然能乐悠悠的过好自己的生活，他自己酿酒即使不好喝，他自己尝试做各种各样的美食，他明白生命不过是某种东西在躯体里短暂的表现形式但也寻求制造长生不老之药，他于生活是开创者是布道者。&emsp;&emsp;我们看看苏轼在生命的最后几年的历程：&emsp;&emsp;一0九三年 妻子逝世；太后逝世，调定州太守。&emsp;&emsp;一0九四年 贬往惠州后谪居惠州。&emsp;&emsp;一0九七年 贬往海南，谪居海南儋州。&emsp;&emsp;一一0一年 北返，往常州。 七月二十八日逝世。&emsp;&emsp;在这种情况下我们再看看这位诗人做了哪些事呢？&emsp;&emsp;在惠州，他还是发挥他喜爱建设的的天性，他与几位太守县令协商建筑了两座大桥，一个在河上，一个在惠州湖上。还有他把无主野坟的骸骨重建为一大冢合葬，他还在城西修了一座放生池。当时他已经是不发俸禄的，没有任何实权，他只是一个热心公益的国民。在惠州的那段时间，他精研佛道儒三家并结合之。在惠州的第二年，他开始自己在一个小山上盖房子，共有二十间，后来这栋房子人称”朝云堂”，苏轼自己的作品里叫”白鹤居”。他用了两年盖好了房子挖好了井种好了树。他的新居落成两月后他被贬海南了。到了海南他又开始盖房子了，但这次他只盖了三间，取名“槟榔庵”。岛上没有墨他便自己制，还险些把房子烧掉。他空闲无事便去乡野采药，考订药的种类，他找到一种古代医术上有，但从没人找到的一种药，他很得意。他写了各种医学笔记，他发现一种荨麻治风湿的方法，很是管用。除去这些，他在海南完成了《东坡志林》的整理，完成了为《尚书》注解，还完成了他和陶潜诗的一百二十四首中最后的十五首。&emsp;&emsp;在苏轼看来高官与平民百姓的不同只是在于做的不同工作，他的内心里是喜欢做一个无忧的隐士，因为他不止一次的表达过对陶渊明的羡慕与敬仰。像苏轼这样的人，为官乃百姓之福，为民方为为自己。所以他才从来没有叹息过为官的命运多舛，他能快乐无所畏惧的过完一生，他能平静的接受从万人之上跌到平民百姓，或许在他看来这也并非是跌落。内心的安宁平静从来不是来源外界，我们现在追逐房子，标榜成功，仿佛只有这些才能带给我们内心的安全与平静。假如有一天你没了工作没有收入，你会惶恐不知所向么？如果会，那么应该怎么去做？你现在做的工作有没有让你感到快乐？我想很多人的答案都是会惶恐，不知道怎么做，不快乐。苏轼用行动告诉我们，解决这些困惑的方法就是保持一颗对这个世界对万物的好奇心，并去学习去钻研，去拥抱每一种不一样的生活，那么你一定会快乐会洒脱，会不去计较得失，因为你在失去的同时也在得到。&emsp;&emsp; &emsp;&emsp;楼下的那家超市突然关门了，昨天还去买了半个西瓜，今天门口就挎着一把大锁了，连同里面的一家面馆一起不再营业了。在看到超市门口一位大妈在便宜处理超市里的一些化妆品，心里还是有一些小小的失落，往后买菜就要走更远一点的路了。不知道别的超市有没有买的旺仔棒棒冰。&emsp;&emsp;我看到一朵花，开在攀爬在生锈的小区围栏上不知名的植物上，周围有很多已经凋零的跟它一样的花朵。它本该开在五月，却在六月里独自绽放，开的格外的鲜艳，像极了你突然转过脸的笑容。&emsp;&emsp;上次买的那双很贵的鞋，才穿了几天，鞋头的线就开了，而且特别容易脏，周末我把它洗了，好像它不像原来的样子了。我们不能说它不值那些钱，可能那双鞋的设计本里就不是让你洗的，穿一两次就已经完成它的任务了，而我并不是能让它那么简单完成任务的人。适合的才是最好的。&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; –2018/6/10 0:54","categories":[{"name":"生活","slug":"生活","permalink":"http://www.chengshaojin.com/categories/生活/"}],"tags":[{"name":"苏轼","slug":"苏轼","permalink":"http://www.chengshaojin.com/tags/苏轼/"}]},{"title":"zabbix总结","date":"2018-05-17T02:54:44.000Z","path":"2018/05/17/zabbix总结/","text":"zabbix应该是一个使用比较广泛的的开源监控软件，它的强大毋庸置疑。我在平时工作中也用到很多zabbix，但也没有系统的去学习总结过，今天就把基础总结一下。 1.zabbix部分名词解释 Zabbix Server : zabbix的控制中心，收集数据、写入数据库都是它的工作。 Zabbix Agent : 部署在被监控服务器上的一个进程，负责个Zabbix Server交互，执行命令。 Host ： 一般就是物理服务器、刀片机、交换机这些实体。 item : 某一个被监控的指标，比如监控cpu负载就是一个item。 Trigger ： 一些逻辑规则的组合，它有三个值：正常、异常、未知。 Action ： 当trigger符合某个值的时候，zabbix会进行的操作，比如发邮件。 2.安装zabbix2.1 安装zabbix-server下面为快速安装脚本12345678910111213141516171819202122232425262728293031323334353637383940414243444546#!/bin/bash#clsn#安装zabbix源、aliyun YUM源curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repocurl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-6.reporpm -ivh http://repo.zabbix.com/zabbix/3.0/rhel/7/x86_64/zabbix-release-3.0-1.el7.noarch.rpm#安装zabbix yum install -y zabbix-server-mysql zabbix-web-mysql#安装启动 mariadb数据库yum install -y mariadb-serversystemctl start mariadb.service#创建数据库mysql -e 'create database zabbix character set utf8 collate utf8_bin;'mysql -e 'grant all privileges on zabbix.* to zabbix@localhost identified by \"zabbix\";'#导入数据zcat /usr/share/doc/zabbix-server-mysql-3.0.17/create.sql.gz|mysql -uzabbix -pzabbix zabbix#配置zabbixserver连接mysqlsed -i.ori '115a DBPassword=zabbix' /etc/zabbix/zabbix_server.conf#添加时区sed -i.ori '18a php_value date.timezone Asia/Shanghai' /etc/httpd/conf.d/zabbix.conf#解决中文乱码yum -y install wqy-microhei-fonts\\cp /usr/share/fonts/wqy-microhei/wqy-microhei.ttc /usr/share/fonts/dejavu/DejaVuSans.ttf#启动服务systemctl start zabbix-serversystemctl start httpd#写入开机自启动chmod +x /etc/rc.d/rc.localcat &gt;&gt;/etc/rc.d/rc.local&lt;&lt;EOFsystemctl start mariadb.servicesystemctl start httpdsystemctl start zabbix-serverEOF#输出信息echo \"浏览器访问 http://`hostname -I|awk '&#123;print $1&#125;'`/zabbix\" 2.2 安装zabbix-agent123456789101112131415161718#!/bin/bash#clsn#安装zabbix源、aliyu nYUM源curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repocurl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-6.reporpm -ivh http://repo.zabbix.com/zabbix/3.0/rhel/7/x86_64/zabbix-release-3.0-1.el7.noarch.rpm#安装zabbix客户端yum install zabbix-agent -ysed -i.ori 's#Server=127.0.0.1#Server=172.16.1.61#' /etc/zabbix/zabbix_agentd.confsystemctl start zabbix-agent.service#写入开机自启动chmod +x /etc/rc.d/rc.localcat &gt;&gt;/etc/rc.d/rc.local&lt;&lt;EOFsystemctl start zabbix-agent.serviceEOF 2.3 检查连通性在服务端安装zabbix-get检测工具1# yum install zabbix-get -y 在服务端测试1234[root@zabbix-server ~]# zabbix_get -s 172.18.23.30 -p 10050 -k \"system.cpu.load[all,avg1]\"0.020000[root@zabbix-server ~]# zabbix_get -s 172.18.23.21 -p 10050 -k \"system.cpu.load[all,avg1]\"0.000000 3.web界面操作3.1 安装zabbix-web浏览器访问 http://172.18.23.30/zabbix/setup.php根据提示，输入数据库密码以及自定义name，依次点击 next step进入登陆界面 账号Admin密码zabbix 注意A大写 3.2 配置监控一台host及items点击 Configuration &gt; Hosts &gt; Create host填完这4项后，监控的主机里没有item，我们用现有的模板来增加监控的item。 点击 Templates &gt; Select (Template OS Linux) &gt; Add &gt; Add可以看到Hosts里多了一行监控的信息。当然也可以一条一条的增加item。在右上角的搜索栏里搜索一个主机，点击 items &gt; Create item 然后填写相关信息。 3.3 添加TriggerTrigger,即触发器，当出现某些情况时，它会发出某些提示行为。在zabbix里可以对一个Trigger定义一些触发的条件，比如某个item的值超过某个阈值，然后定义这个Trigger触发后该干什么。我们来配一个Trigger，当CPU负载超过某个阈值时，会触发这个Trigger。点击 “Configuration &gt; Hosts &gt; Triggers &gt; Create trigger”Expression是服务器CPU负载的item。添加完在Trigger里可以看到刚刚添加的triger 3.4 设置Action首先配置zabbix使用的邮件服务器。点击 “Administration &gt; Media types &gt; Email”再新建一个Action。 “Configuration &gt; action &gt; Create action”配置Action 、Conditions、Operations。至此，报警动作就已经配置好了。未完，待更。","categories":[{"name":"技术","slug":"技术","permalink":"http://www.chengshaojin.com/categories/技术/"}],"tags":[{"name":"zabbix","slug":"zabbix","permalink":"http://www.chengshaojin.com/tags/zabbix/"}]},{"title":"crush设计：SSD、SATA盘的混合使用","date":"2018-05-05T01:13:41.000Z","path":"2018/05/05/crush设计：SSD、SATA盘的混合使用/","text":"虽然固态硬盘成本在大幅降低，但相比机械硬盘依然成本较高。那么为了充分使用ssd与sata,就有这样两种场景：一种是在一套ceph里，对IO性能需求比较高的数据用ssd，对IO性能需求较低的使用普通sata盘。比如常见的云环境里，虚拟机启动盘用ssd的存储池，快照备份等用sata的存储池。还有一种场景是在多副本的情况下，把主副本放在ssd的bucket里，其他副本放在sata设备上。这两种场景都可以通过自定义设计crushmap文件来达到。下面我结合openstack，详细说明应用的方法。 一、设计openstack中cinder ceph多后端安装openstack+ceph由于资源有限，我在一台机器上实验。还是使用kolla,搭建一套all-in-one的openstack环境。有两块盘，一块sata /dev/sdb ,一块ssd /dev/sdc 。在global文件里打开ceph。给两块盘打ceph标签。12parted /dev/sdb -s -- mklabel gpt mkpart KOLLA_CEPH_OSD_BOOTSTRAP 1 -1parted /dev/sdc -s -- mklabel gpt mkpart KOLLA_CEPH_OSD_CACHE_BOOTSTRAP 1 -1 设置副本数12345编辑/etc/kolla/config/ceph.conf文件[root@queens ~]# cat /etc/kolla/config/ceph.conf [global]osd pool default size = 1osd pool default min size = 1 配置好其他项就可以部署了，具体配置哪些可以看我前面的博客。部署完成以后增加一个cinder后端，然后设计crushmap了。 编辑/etc/kolla/cinder-volume/cinder.conf123456789101112131415161718增加以下配置[DEFAULT]enabled_backends = rbd-1,ssd[ssd]volume_driver = cinder.volume.drivers.rbd.RBDDrivervolume_backend_name = ssdrbd_pool = volumes-cacherbd_ceph_conf = /etc/ceph/ceph.confrbd_flatten_volume_from_snapshot = falserbd_max_clone_depth = 5rbd_store_chunk_size = 4rados_connect_timeout = 5rbd_user = cinderrbd_secret_uuid = b89a2a40-c009-47da-ba5b-7b6414a1f759 #通过uuidgen生成report_discard_supported = Trueimage_upload_use_cinder_backend = True重启cinder-volume服务# docker restart cinder_volume 设计crush获取当前的crush map12# docker exec -it ceph_mon bash# ceph osd getcrushmap -o crushmap.old 反编译crush mapcrushmap文件是一个二进制文件，通过crushtool反编译为文本文件。1# crushtool -d crushmap.old -o crushmap.new 编辑crush map 文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596# vi crushmap.new# begin crush maptunable choose_local_tries 0tunable choose_local_fallback_tries 0tunable choose_total_tries 50tunable chooseleaf_descend_once 1tunable chooseleaf_vary_r 1tunable chooseleaf_stable 1tunable straw_calc_version 1tunable allowed_bucket_algs 54# devicesdevice 0 device0device 1 osd.1 class hdddevice 2 osd.2 class hdd# typestype 0 osdtype 1 hosttype 2 chassistype 3 racktype 4 rowtype 5 pdutype 6 podtype 7 roomtype 8 datacentertype 9 regiontype 10 root# bucketshost queens &#123; # sata的host层级 id -1 # 设置唯一id id -2 class hdd # do not change unnecessarily # weight 2.000 alg straw2 hash 0 # rjenkins1 item osd.1 weight 1.000 #在主机quees里的sata盘 的osd&#125;root default &#123; # sata的root层级 id -3 # do not change unnecessarily id -4 class hdd # do not change unnecessarily # weight 2.000 alg straw2 hash 0 # rjenkins1 item queens weight 1.000&#125;host queens-ssd &#123; #新建ssd的host层级 id -5 # do not change unnecessarily id -6 class hdd # do not change unnecessarily # weight 2.000 alg straw2 hash 0 # rjenkins1 item osd.2 weight 1.000 #在主机quees里的ssd盘的osd&#125;root ssd &#123; #新建一个root层级 命名为ssd id -7 # do not change unnecessarily id -8 class hdd # do not change unnecessarily # weight 2.000 alg straw2 hash 0 # rjenkins1 item queens-ssd weight 1.000&#125;# rulesrule replicated_rule &#123; id 0 type replicated min_size 1 max_size 10 step take default step chooseleaf firstn 0 type host step emit&#125;rule disks &#123; #sata rule id 1 type replicated min_size 1 max_size 10 step take default step chooseleaf firstn 0 type host step emit&#125;rule ssd &#123; # 新建ssd rule id 2 type replicated min_size 1 max_size 10 step take default step chooseleaf firstn 0 type host step emit&#125;# end crush map 编译crushmap1# crushtool -c crushmap.new -o crushmap.bin 把新的crushmap应用到ceph里1ceph osd setcrushmap -i crushmap.bin 查看crush结构，确认新的crushmap生效12345678(ceph-mon)[root@queens /]# ceph osd treeID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -7 1.00000 root ssd -5 1.00000 host queens-ssd 2 ssd 1.00000 osd.2 up 1.00000 1.00000 -3 1.00000 root default -1 1.00000 host queens 1 hdd 1.00000 osd.1 up 1.00000 1.00000 查看rule1234(ceph-mon)[root@queens /]# ceph osd crush rule lsreplicated_ruledisksssd 创建ssd的存储池并指定rule1# ceph osd pool create SSD 64 64 ssd 查看rule是否生效123456(ceph-mon)[root@queens /]# ceph osd dump |grep -i poolpool 1 'images' replicated size 1 min_size 1 crush_rule 1 object_hash rjenkins pg_num 128 pgp_num 128 last_change 39 flags hashpspool stripe_width 0 application rbdpool 2 'volumes' replicated size 1 min_size 1 crush_rule 1 object_hash rjenkins pg_num 128 pgp_num 128 last_change 40 flags hashpspool stripe_width 0 application rbdpool 3 'backups' replicated size 1 min_size 1 crush_rule 1 object_hash rjenkins pg_num 128 pgp_num 128 last_change 41 flags hashpspool stripe_width 0 application rbdpool 4 'vms' replicated size 1 min_size 1 crush_rule 1 object_hash rjenkins pg_num 128 pgp_num 128 last_change 42 flags hashpspool stripe_width 0 application rbdpool 9 'SSD' replicated size 1 min_size 1 crush_rule 2 object_hash rjenkins pg_num 32 pgp_num 32 last_change 56 flags hashpspool stripe_width 0 创建两个cinder卷类型1234567891011# cinder type-create SATA# cinder type-create SSD[root@queens ~]# cinder type-list/usr/lib/python2.7/site-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.22) or chardet (2.2.1) doesn't match a supported version! RequestsDependencyWarning)+--------------------------------------+------+-------------+-----------+| ID | Name | Description | Is_Public |+--------------------------------------+------+-------------+-----------+| 483595f4-5882-4b84-8108-3a9df703d1cb | SSD | - | True || 532ad78f-2205-4b1a-888d-b4670d5f4463 | SATA | - | True |+--------------------------------------+------+-------------+-----------+ 设置卷类型的key键值1234567891011# cinder type-key SSD set volume_backend_name=ssd# cinder type-key SATA set volume_backend_name=rbd-1[root@queens ~]# cinder extra-specs-list/usr/lib/python2.7/site-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.22) or chardet (2.2.1) doesn't match a supported version! RequestsDependencyWarning)+--------------------------------------+------+----------------------------------+| ID | Name | extra_specs |+--------------------------------------+------+----------------------------------+| 483595f4-5882-4b84-8108-3a9df703d1cb | SSD | &#123;'volume_backend_name': 'ssd'&#125; || 532ad78f-2205-4b1a-888d-b4670d5f4463 | SATA | &#123;'volume_backend_name': 'rbd-1'&#125; |+--------------------------------------+------+----------------------------------+ 这是创建云硬盘就可以选择sata还是ssd了 二.主备存储方案由于没有硬件资源可以搭建三幅本的ceph，在这里我只说一下具体方法。主备存储方案的crush设计跟上面的一样，只需在rule上做一些修改即可。ceph的读写流程：1234567891011121314151617假设我们上面的是三幅本，我们沿用上面的crush,创建新的crush rule。rule pg &#123; id 3 type replicated min_size 1 max_size 10 step take ssd #指定入口bucket为ssd step chooseleaf firstn 1 type host #从ssd bucket搜索一个合适的osd存储主副本 step emit step take default #指定入口bucket为sata step chooseleaf firstn -1 type host #从default bucket搜索其他副本所需的osd来存储 step emit&#125;编译应用并创建一个名为pg的存储池，指定pg rule,验证pg pool 的PG分布。# ceph pg dump | grep '^3\\.' | awk 'BEGIN&#123;print \"PG_id\",\"\\t\",\"copy_set\"&#125;&#123;print $1,\"\\t\",$15&#125;' | less 拓展：除了利用crush来实现主备存储，还可以通过osd的亲和性实现主备存储。调整osd的primary affinity的值，只要把sata设备对应的osd的值设置为0 ，这些osd就不会成为主副本，读写都只落到ssd对应的osd上。","categories":[{"name":"技术","slug":"技术","permalink":"http://www.chengshaojin.com/categories/技术/"}],"tags":[{"name":"ceph","slug":"ceph","permalink":"http://www.chengshaojin.com/tags/ceph/"}]},{"title":"用cobbler安装操作系统","date":"2018-04-18T08:17:39.000Z","path":"2018/04/18/用cobbler安装操作系统/","text":"cobbler是一个linux服务器安装的服务，可以通过pxe、kickstart自动、快速、批量的安装系统，并可以管理dhcp、dns等。cobbler有一套web界面管理工具（cobbler-web），也可以使用命令行管理。 Cobbler集成的服务有： DHCP服务管理 DNS服务管理 电源管理 Ｋickstart服务 YUM仓库管理 TFTP httpd服务1.安装cobbler1.1 环境信息12345678910[root@cobbler ~]# cat /etc/redhat-release CentOS Linux release 7.2.1511 (Core) [root@cobbler ~]# uname -r3.10.0-327.el7.x86_64[root@cobbler ~]# getenforceDisabled[root@cobbler ~]# systemctl status firewalld● firewalld.service - firewalld - dynamic firewall daemon Loaded: loaded (/usr/lib/systemd/system/firewalld.service; disabled; vendor preset: enabled) Active: inactive (dead) yum源说明：12# curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo# curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo 1.2 yum安装cobbler12345678910111213141516# yum -y install cobbler cobbler-web dhcp tftp-server pykickstart httpd# systemctl start httpd.service# systemctl start cobblerd.service# cobbler check[root@cobbler ~]# cobbler checkThe following are potential configuration items that you may want to fix:1 : The 'server' field in /etc/cobbler/settings must be set to something other than localhost, or kickstarting features will not work. This should be a resolvable hostname or IP for the boot server as reachable by all machines that will use it.2 : For PXE to be functional, the 'next_server' field in /etc/cobbler/settings must be set to something other than 127.0.0.1, and should match the IP of the boot server on the PXE network.3 : change 'disable' to 'no' in /etc/xinetd.d/tftp4 : Some network boot-loaders are missing from /var/lib/cobbler/loaders, you may run 'cobbler get-loaders' to download them, or, if you only want to handle x86/x86_64 netbooting, you may ensure that you have installed a *recent* version of the syslinux package installed and can ignore this message entirely. Files in this directory, should you want to support all architectures, should include pxelinux.0, menu.c32, elilo.efi, and yaboot. The 'cobbler get-loaders' command is the easiest way to resolve these requirements.5 : enable and start rsyncd.service with systemctl6 : debmirror package is not installed, it will be required to manage debian deployments and repositories7 : ksvalidator was not found, install pykickstart8 : The default password used by the sample templates for newly installed machines (default_password_crypted in /etc/cobbler/settings) is still set to 'cobbler' and should be changed, try: \"openssl passwd -1 -salt 'random-phrase-here' 'your-password-here'\" to generate new one9 : fencing tools were not found, and are required to use the (optional) power management features. install cman or fence-agents to use themRestart cobblerd and then run 'cobbler sync' to apply changes. 1.3 解决cobbler check的报错12345678910111213# sed -i 's/server: 127.0.0.1/server: 192.168.1.139/' /etc/cobbler/settings# sed -i 's/next_server: 127.0.0.1/next_server: 192.168.1.139/' /etc/cobbler/settings# sed -i 's/manage_dhcp: 0/manage_dhcp: 1/' /etc/cobbler/settings# sed -i 's/pxe_just_once: 0/pxe_just_once: 1/' /etc/cobbler/settings# sed -ri \"/default_password_crypted/s#(.*: ).*#\\1\\\"`openssl passwd -1 -salt 'oldboy' '123456'`\\\"#\" /etc/cobbler/settings# sed -i 's#yes#no#' /etc/xinetd.d/tftp# systemctl start rsyncd# systemctl enable rsyncd# systemctl enable tftp.socket# systemctl start tftp.socket# systemctl restart cobblerd.service# sed -i.ori 's#192.168.1#172.16.1#g;22d;23d' /etc/cobbler/dhcp.template# cobbler sync 下载所需软件包：1# cobbler get-loaders 启动rsync服务12# systemctl start rsyncd.service# systemctl enable rsyncd.service 修改安装完后的root密码1# openssl passwd -1 -salt 'CLNS' '123456' 第一个引号内的是随机的，第二个引号是你的密码管理dhcp1# sed -i 's/manage_dhcp: 0/manage_dhcp: 1/' /etc/cobbler/settings 防止重装1# sed -i 's/pxe_just_once: 0/pxe_just_once: 1/' /etc/cobbler/settings 注意： 修改完成之后要使用cobbler sync 进行同步，否则不生效。再次检查语法：123456[root@cobbler yum.repos.d]# cobbler checkThe following are potential configuration items that you may want to fix:1 : debmirror package is not installed, it will be required to manage debian deployments and repositories2 : ksvalidator was not found, install pykickstart3 : fencing tools were not found, and are required to use the (optional) power management features. install cman or fence-agents to use themRestart cobblerd and then run 'cobbler sync' to apply changes. 重启所有服务12345systemctl restart httpd.servicesystemctl restart cobblerd.servicesystemctl restart dhcpd.servicesystemctl restart rsyncd.servicesystemctl restart tftp.socket 1.4 cobbler-web界面的操作浏览器访问：https://192.168.1.139/cobbler_web账号密码默认为cobbler把镜像添加在虚拟机CD/DVD，然后mount /dev/cdrom /mnt在界面导入镜像（import DVD）,写入prefix选择arch与breed, path为 /mnt 然后run在命令行查看进程，三个rsync小时表示导入完成123456[root@cobbler ~]# ps -ef |grep rsyncroot 12026 1 0 19:04 ? 00:00:00 /usr/bin/rsync --daemon --no-detachroot 13554 11778 12 19:51 ? 00:00:06 rsync -a /mnt/ /var/www/cobbler/ks_mirror/CentOS7.4-x86_64 --progressroot 13555 13554 0 19:51 ? 00:00:00 rsync -a /mnt/ /var/www/cobbler/ks_mirror/CentOS7.4-x86_64 --progressroot 13556 13555 33 19:51 ? 00:00:17 rsync -a /mnt/ /var/www/cobbler/ks_mirror/CentOS7.4-x86_64 --progressroot 13590 10759 0 19:52 pts/1 00:00:00 grep --color=auto rsync 启动一台新的虚拟机进行pxe安装系统。 界面操作有很多功能，可定制化安装操作系统、自定义安装系统等等。","categories":[{"name":"技术","slug":"技术","permalink":"http://www.chengshaojin.com/categories/技术/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://www.chengshaojin.com/tags/Linux/"}]},{"title":"openstack queens版本multi-attach实践","date":"2018-04-11T09:22:00.000Z","path":"2018/04/11/openstack queens版本multi-attach实践/","text":"Openstack第17个版本Queens发布以来，最引人注目的莫过于cinder后端volume multiattach功能了，这个功能可以把一个volume同时挂载给多个不同的虚拟机，如果其中一个虚拟机宕了，其他的虚拟机可以接管这个卷并正常访问。该功能解决了很多客户的实际需求，是个非常实用的功能。下面我用kolla搭建一个all-in-one的queens环境，cinder后端使用lvm。 安装openstack queens1.准备环境123456[root@queens ~]# cat /etc/redhat-release CentOS Linux release 7.2.1511 (Core) [root@queens ~]# uname -r3.10.0-327.el7.x86_64[root@queens ~]# hostnamequeens 关闭firewall、selinux、NetworkManager查看是否开启虚拟化1[root@queens ~]# egrep \"vmx|svm\" /proc/cpuinfo 安装基础软件包12[root@queens ~]# yum install epel-release[root@queens ~]# yum install axel vim git curl wget lrzsz gcc python-devel python-pip 2.安装配置docker12# wget -O /etc/yum.repos.d/docker-ce.repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo# yum install -y docker-ce 12345678# mkdir /etc/systemd/system/docker.service.d# tee /etc/systemd/system/docker.service.d/kolla.conf &lt;&lt; 'EOF'[Service]MountFlags=sharedEOF# vim /usr/lib/systemd/system/docker.service# ExecStart=/usr/bin/dockerdExecStart=/usr/bin/dockerd --registry-mirror=http://f2d6cb40.m.daocloud.io --storage-driver=overlay2 12345# systemctl daemon-reload# systemctl restart docker# systemctl enable dockerCreated symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.# systemctl status docker 3.安装ansible1# yum -y install ansible -y 4.下载kolla-ansible,安装配置1234# git clone https://github.com/openstack/kolla-ansible -b stable/queens# cd kolla-ansible/# cp -r etc/kolla/ /etc/kolla/# pip install . -i https://pypi.tuna.tsinghua.edu.cn/simple 修改globals.yml文件1234567891011121314151617[root@queens ~]# egrep \"^[^#]\" /etc/kolla/globals.yml---kolla_install_type: \"source\"openstack_release: \"queens\"kolla_internal_vip_address: \"192.168.192.129\"docker_namespace: \"kolla\"network_interface: \"eno16777736\"neutron_external_interface: \"eno33554960\"enable_cinder: \"yes\"enable_cinder_backend_lvm: \"yes\"enable_haproxy: \"no\"cinder_volume_group: \"cinder-volumes\"nova_compute_virt_type: \"qemu\"tempest_image_id:tempest_flavor_ref_id:tempest_public_network_id:tempest_floating_network_name: 5.配置lvm12# pvcreate /dev/sdb# vgcreate cinder-volume /dev/sdb 6.安装kolla及部署openstack123456# kolla-genpwd修改/etc/kolla/passwords.yml文件# kolla-ansible prechecks# kolla-ansible pull# kolla-ansible deploy# kolla-ansible post-deploy 6.openstack环境配置1234567安装openstack client# pip install python-openstackclient修改init-runonce文件，设置创建网络的ip池加载环境变量# source /etc/kolla/admin-openrc.sh初始化# cd /usr/share/kolla-ansibale &amp;&amp; ./init-runonce multiattach实践1.创建multiattach的卷类型并设置1234567[root@queens ~]# cinder type-create multiattach+--------------------------------------+-------------+-------------+-----------+| ID | Name | Description | Is_Public |+--------------------------------------+-------------+-------------+-----------+| ea3205d6-e3a0-4dd4-974a-20fab2830bf7 | multiattach | - | True |+--------------------------------------+-------------+-------------+-----------+[root@queens ~]# cinder type-key multiattach set multiattach=\"&lt;is&gt; True\" 2.创建一个1G的volume,使用刚刚创建的卷类型1234567891011121314151617181920212223242526272829[root@queens ~]# cinder create 1 --name multiattach_volume --volume-type ea3205d6-e3a0-4dd4-974a-20fab2830bf7+--------------------------------+--------------------------------------+| Property | Value |+--------------------------------+--------------------------------------+| attachments | [] || availability_zone | nova || bootable | false || consistencygroup_id | None || created_at | 2018-04-11T09:13:57.000000 || description | None || encrypted | False || id | d93e1565-6dd2-442a-a017-f1dc5cc91d86 || metadata | &#123;&#125; || migration_status | None || multiattach | True || name | multiattach_volume || os-vol-host-attr:host | queens@lvm-1#lvm-1 || os-vol-mig-status-attr:migstat | None || os-vol-mig-status-attr:name_id | None || os-vol-tenant-attr:tenant_id | fbe246bc04a146928d8ecaf0e32d8eec || replication_status | None || size | 1 || snapshot_id | None || source_volid | None || status | creating || updated_at | 2018-04-11T09:13:57.000000 || user_id | 7593374948ec463fa09a71a24c82c69e || volume_type | multiattach |+--------------------------------+--------------------------------------+ 3.创建两个虚拟机1234567[root@queens ~]# nova list+--------------------------------------+--------+--------+------------+-------------+--------------------+| ID | Name | Status | Task State | Power State | Networks |+--------------------------------------+--------+--------+------------+-------------+--------------------+| 4203f1d1-1511-437d-bd0d-01b23bf01661 | test-1 | ACTIVE | - | Running | public1=10.0.2.150 || dd773d0e-fe5e-47d2-b5b0-aafbaab5b41b | test-2 | ACTIVE | - | Running | public1=10.0.2.157 |+--------------------------------------+--------+--------+------------+-------------+--------------------+ 4.挂载multiattach_volume到两个虚拟机1234567891011121314151617181920[root@queens ~]# nova volume-attach 4203f1d1-1511-437d-bd0d-01b23bf01661 d93e1565-6dd2-442a-a017-f1dc5cc91d86+----------+--------------------------------------+| Property | Value |+----------+--------------------------------------+| device | /dev/vdb || id | d93e1565-6dd2-442a-a017-f1dc5cc91d86 || serverId | 4203f1d1-1511-437d-bd0d-01b23bf01661 || volumeId | d93e1565-6dd2-442a-a017-f1dc5cc91d86 |+----------+--------------------------------------+[root@queens ~]# nova volume-attach dd773d0e-fe5e-47d2-b5b0-aafbaab5b41b d93e1565-6dd2-442a-a017-f1dc5cc91d86+----------+--------------------------------------+| Property | Value |+----------+--------------------------------------+| device | /dev/vdb || id | d93e1565-6dd2-442a-a017-f1dc5cc91d86 || serverId | dd773d0e-fe5e-47d2-b5b0-aafbaab5b41b || volumeId | d93e1565-6dd2-442a-a017-f1dc5cc91d86 |+----------+--------------------------------------+ 5.查看卷信息，可以看到被挂载在两个虚拟机上12345678910111213141516171819202122232425262728293031[root@queens ~]# cinder show d93e1565-6dd2-442a-a017-f1dc5cc91d86+--------------------------------+----------------------------------------------------------------------------------+| Property | Value |+--------------------------------+----------------------------------------------------------------------------------+| attached_servers | ['4203f1d1-1511-437d-bd0d-01b23bf01661', 'dd773d0e-fe5e-47d2-b5b0-aafbaab5b41b'] || attachment_ids | ['26631abb-cfb4-48a8-9da2-30bb47f26905', 'fc0cbde4-2496-4123-a020-a409ac93b83d'] || availability_zone | nova || bootable | false || consistencygroup_id | None || created_at | 2018-04-11T09:13:57.000000 || description | None || encrypted | False || id | d93e1565-6dd2-442a-a017-f1dc5cc91d86 || metadata | attached_mode : rw || migration_status | None || multiattach | True || name | multiattach_volume || os-vol-host-attr:host | queens@lvm-1#lvm-1 || os-vol-mig-status-attr:migstat | None || os-vol-mig-status-attr:name_id | None || os-vol-tenant-attr:tenant_id | fbe246bc04a146928d8ecaf0e32d8eec || replication_status | None || size | 1 || snapshot_id | None || source_volid | None || status | in-use || updated_at | 2018-04-11T09:16:02.000000 || user_id | 7593374948ec463fa09a71a24c82c69e || volume_type | multiattach |+--------------------------------+----------------------------------------------------------------------------------进入两个虚拟机系统可以看到该磁盘信息。 6.测试HA1.进入第一个虚拟机系统分区、格式化、挂载到/mnt并写入文件。2.模拟虚拟机故障，然后进入第二个虚拟机系统挂载分区，查看文件。可以看到第一个虚拟机写入的文件，并能继续写入。 7.Known issues and limitations Retyping an in-use volume from a multiattach-capable type to a non-multiattach-capable type, or vice-versa, is not supported. It is not recommended to retype an in-use multiattach volume if that volume has more than one active read/write attachment.","categories":[{"name":"技术","slug":"技术","permalink":"http://www.chengshaojin.com/categories/技术/"}],"tags":[{"name":"openstack","slug":"openstack","permalink":"http://www.chengshaojin.com/tags/openstack/"}]},{"title":"那些看一眼就惊艳了时光的诗句","date":"2018-03-31T13:17:11.000Z","path":"2018/03/31/那些一眼就惊艳了时光的诗句/","text":"十里青山远，潮平路带沙。数声啼鸟怨年华。又是凄凉时候、在天涯。白露收残暑，清风衬晚霞。绿杨堤畔闹荷花。记得年时沽酒、那人家。 ——《南柯子 十里青山远》仲殊 兜兜转转两三年，如今三年不似那三年。偷的半日的时光，在那十里的果园里笑开了花。 把酒祝东风，且共从容。垂杨紫陌洛城东。总是当时携手处，游遍芳丛。聚散苦匆匆，此恨无穷。今年花胜去年红。可惜明年花更好，知与谁同？ ——《浪淘沙 把酒祝东风》欧阳修 年年岁岁花相似，岁岁年年人不同。愿未来如你所期，不负时光不忘初心。 沧海蓝田共烟霞，珠玉冷暖在谁家。金人莫论兴衰事，铜仙惯乘来去车。孤艇酒酣焚经典，高枝月明判凤鸦。蓬莱枯死三千树，为君重满碧桃花。 ——摘自木心作品《西班牙三棵树》 蓬莱枯死三千树，为君重满碧桃花。 哀利丝•霍珈走过来悄悄说，如果有人欺侮你，你就种一棵树——这也太美丽得犬儒主义的春天似的；我是，这样想，当谁欺侮了谁时，神灵便暗中播一棵树，森林是这样形成的，谁树即谁人，却又都不知道。 一别都门三改火，天涯踏尽红尘。依然一笑作春温。无波真古井，有节是秋筠。惆怅孤帆连夜发，送行淡月微云。樽前不用翠眉颦。人生如逆旅，我亦是行人。 ——《临江仙·送钱穆父》苏轼 常羡人间琢玉郎，天应乞与点酥娘。尽道清歌传皓齿，风起，雪飞炎海变清凉。万里归来颜愈少，微笑，笑时犹带岭梅香。试问岭南应不好，却道：此心安处是吾乡。 ——《定风波 南海归赠王定国侍人寓娘》苏轼 细雨斜风作晓寒，淡烟疏柳媚晴滩。入淮清洛渐漫漫。雪沫乳花浮午盏，蓼茸蒿笋试春盘。人间有味是清欢。 ——《浣溪沙 细雨斜风作晓寒》苏轼 人间有味是清欢 瀑布的水逆流而上，蒲公英种子从远处飘回，聚成伞的模样，太阳从西边升起，落向东方。 子弹退回枪膛，运动员回到起跑线上，我交回录取通知书，忘了十年寒窗。 厨房里飘来饭菜的香，你把我的卷子签好名字，关掉电视，帮我把书包背上。 你还在我身旁 。 ——《你还在我身旁》香港中文大学学生作品","categories":[{"name":"生活","slug":"生活","permalink":"http://www.chengshaojin.com/categories/生活/"}],"tags":[{"name":"诗句","slug":"诗句","permalink":"http://www.chengshaojin.com/tags/诗句/"}]},{"title":"openstack配置lvm与nfs的cinder多后端","date":"2018-03-14T08:39:14.000Z","path":"2018/03/14/openstack配置lvm与nfs的cinder多后端/","text":"不知道自己习惯还是怎么，潜意识里总觉得openstack与ceph是最配的一对，平时测试自己玩都是ceph做后端，最近有个客户的POC，明确要求用lvm，而且他们用的存储还都是nfs，所以我就测试了下用lvm与nfs做cinder的多后端，以此记录。测试openstack环境还是用kolla部署的pika版,先使用lvm做cinder后端，然后再配置nfs。 在每个存储节点准备一块单独的盘创建 volume group12# pvcreate /dev/sdb# vgcreate cinder-volume /dev/sdb 在global文件中打开lvmenable_cinder_backend_lvm: &quot;yes&quot; lvm的配置就这么多，要注意的是你的vg的名字必须和global文件中cinder_volume_group的名字一致。cat /etc/kolla/globals.yml | grep cinder_volume cinder_volume_group: “cinder-volumes”在cinder后端不使用ceph的时候，global文件里默认glance存储在本地文件里。 配置nfs安装nfs,确保nfs与rpcbind服务正常,确认nfs设备创建成功，能够挂载并有读写权限，可以在/etc/exports文件中这么写：/mnt/share 192.168.226.0/24(rw,sync,no_root_squash)1mount -t nfs 192.168.226.128:/mnt/share /nfs_shares 在cinder.conf文件中做如下配置：1234567891011vi /etc/kolla/cinder-volume/cinder.conf[DEFAULT]enabled_backends = nfs[nfs]volume_backend_name = nfsvolume_driver = cinder.volume.drivers.nfs.NfsDrivernas_host = 192.168.226.128nas_share_path = /mnt/sharenfs_mount_attempts = 3 重启cinder_volume容器，查看日志看是否加载成功。 创建两个cinder卷类型12345678910# source /etc/kolla/admin-openrc.sh# cinder type-create lvm# cinder type-create nfs# cinder type-list+--------------------------------------+------+-------------+-----------+| ID | Name | Description | Is_Public |+--------------------------------------+------+-------------+-----------+| c9936b51-aaf1-4822-80fe-f0a1d91196bf | lvm | - | True || cf3a55be-a32c-412d-9175-33e1bd33f45b | nfs | - | True |+--------------------------------------+------+-------------+-----------+ 设置卷类型的key键值123456789# cinder type-key lvm set volume_backend_name=lvm-1# cinder type-key nfs set volume_backend_name=nfs-1# cinder extra-specs-list+--------------------------------------+------+----------------------------------+| ID | Name | extra_specs |+--------------------------------------+------+----------------------------------+| c9936b51-aaf1-4822-80fe-f0a1d91196bf | lvm | &#123;'volume_backend_name': 'lvm-1'&#125; || cf3a55be-a32c-412d-9175-33e1bd33f45b | nfs | &#123;'volume_backend_name': 'nfs-1'&#125; |+--------------------------------------+------+----------------------------------+ 在dashboard创建卷的时候就可以选择使用哪种后端： 测试12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# cinder create --volume_type lvm --display_name volume_lvm 1+--------------------------------+--------------------------------------+| Property | Value |+--------------------------------+--------------------------------------+| attachments | [] || availability_zone | nova || bootable | false || consistencygroup_id | None || created_at | 2018-03-14T17:31:28.000000 || description | None || encrypted | False || id | a17fde61-02a3-48ff-ba10-98908770704b || metadata | &#123;&#125; || migration_status | None || multiattach | False || name | volume_lvm || os-vol-host-attr:host | None || os-vol-mig-status-attr:migstat | None || os-vol-mig-status-attr:name_id | None || os-vol-tenant-attr:tenant_id | c0875c01f9394cffac2f83c43bce2e80 || replication_status | None || size | 1 || snapshot_id | None || source_volid | None || status | creating || updated_at | None || user_id | 586c61eb3200477b9c03a22cd9baea50 || volume_type | lvm |+--------------------------------+--------------------------------------+# cinder create --volume_type nfs --display_name volume_nfs 1+--------------------------------+--------------------------------------+| Property | Value |+--------------------------------+--------------------------------------+| attachments | [] || availability_zone | nova || bootable | false || consistencygroup_id | None || created_at | 2018-03-14T17:33:13.000000 || description | None || encrypted | False || id | 360865c0-90d3-4c69-b463-c5771d39962e || metadata | &#123;&#125; || migration_status | None || multiattach | False || name | volume_nfs || os-vol-host-attr:host | None || os-vol-mig-status-attr:migstat | None || os-vol-mig-status-attr:name_id | None || os-vol-tenant-attr:tenant_id | c0875c01f9394cffac2f83c43bce2e80 || replication_status | None || size | 1 || snapshot_id | None || source_volid | None || status | creating || updated_at | None || user_id | 586c61eb3200477b9c03a22cd9baea50 || volume_type | nfs |+--------------------------------+--------------------------------------+ 到此cinder的lvm与nfs多后端配置完成。","categories":[{"name":"技术","slug":"技术","permalink":"http://www.chengshaojin.com/categories/技术/"}],"tags":[{"name":"openstack","slug":"openstack","permalink":"http://www.chengshaojin.com/tags/openstack/"}]},{"title":"2018年春节前最后一天上班的晚上","date":"2018-02-09T15:04:34.000Z","path":"2018/02/09/2018年春节前最后一天上班的晚上/","text":"晚上收拾了下明天回家的行李，想想一年就这样又过去了，是不是应该总结一下过去一年的得失、荣辱、希望和失望，这样也给自己一种仪式感，写一写感悟，谈一谈一年中的改变，想一想18年应该做哪些事，提醒自己不负时光，能光明前进。 慢慢静下心来，匆匆2018年也已经是2月份了，这两月工作不是特别的忙，上班时间基本也是自己安排，也让自己放松了很多，下班后都是看小说，没干什么正事，熬了几个夜晚看完了曾国藩三部曲，导致自己最近总是爱琢磨别人说的话到底有什么深层意思，其实也没有什么意思，平常人哪有那么高深莫测，没干啥事反而让自己很累。后来就自己劝自己我等凡夫俗子就别想那么多了，别人一张口就知道人家心里想什么的内家本事，没个几十年的功力哪能轻易练成，哈哈。开个玩笑，还是说说自己到底干了什么吧。 关于工作这一年发生最重要的事就是在十月份换了工作，这是我没有想到的，虽然依然在同一个行业，但在一家公司呆的时间太短总不是一件太好的事，再说当时的领导对初入职场的我在做人做事各方面有很大的帮助，很多事都是亲历亲为的教我们，我很感谢他，希望他在新的一年工作顺利，身体健康。然后因为很多其他原因，当然也有一部分我自己的原因，就匆匆辞职了换了一份工作，依然在做openstack运维实施方面的工作。其实当时刚毕业的时候自己真的是十分的迷茫，不知道自己能干什么，看到openstack正如日中天，就开始学习，现在也能因此谋得一份工作也感到十分的庆幸。即使openstack呈下落趋势，任何开源项目都有其上升期和下降期，但是现在也会因为工作需要去不断的学习新的知识，不再因为不知道自己能干什么想干什么而迷茫，我想这应该是自己这一年最大的收获吧。 其实工作中能识人然后正确的待人也是工作实力的一部分，刚出校门的我，因为说话做事太直，吃了不少的亏，吃一斩长一智嘛，现在想来其实吃吃亏也是蛮好的。让自己认识到自己情商的严重不足，应该注意慢慢提高。我也一直在提醒自己要不忘初心，做人做事要有自己的底线，不要变得太圆滑，成为自己以前讨厌的人。 关于生活离开校园生活变得非常规律了，以前在学校总是抽烟，现在也不抽了，其实我是没有烟瘾的，所以说戒就戒了。自从离开学校开始租房子了，周末有时间就开始自己做饭了，这一点是我非常开心的，生活的情趣来源于不断的创造，尤其当你做出一盘色香味俱全的菜的时候，那种成就感是不由而生的。由此也感受到，其实每天做饭也是一件非常耗时间的，而且加上洗锅也是比较累的，做饭的时候信心满满，但是吃完就不想洗锅了，哈哈，争取以后就买自动洗碗机了，看了网上的评论，说是还不错。一年也没怎么出去旅行，大学的时候没钱，反而经常出去玩，在北京一年多，连北京市也没怎么玩过，国庆放假的时候计划好了出去，结果加了四天班，也再没心情出去了，以后心态有待改正，不工作的时候还是应该多出去看看的，尤其一些自然风景区，大自然自带一种开阔心胸的功能。唯一遗憾的就是自从毕业就很少再踢球了，这项占据了大学最多时间的运动，现在却很少去踢了。我也能感觉到身体机能大不如以前了，总想着去锻炼，但都是时间用来做了其他事了，其实归根结底还是太懒了。很怀念当年参加大学生运动会的时候跟球队吃住在一起的日子以及每天下午在操场练球的日子。 2018年计划要做的：1.学驾照。虽说人工智能的突飞猛进已经实现了无人驾驶，但还是想体验驾驶的乐趣。一定要做的。2.学习python语言。简直刻不容缓啊。3.能初学投资，虽然没钱，但我也有一颗想实现财富自由的心啊。4.能读20本以上的书。20本应该很少了，比不了大神，自己开心就好，貌似最近没咋上心工作，下班已经读了三四本了吧，主要是必须要完成，没有上线，不要辜负了自己的kindle。哈哈。5.提高自己的厨艺，会做菜品的数量有待丰富，没有多大要求，能满足家常就好了。 絮絮叨叨没说啥正事，感觉都是废话啊。。。","categories":[{"name":"生活","slug":"生活","permalink":"http://www.chengshaojin.com/categories/生活/"}],"tags":[{"name":"生活","slug":"生活","permalink":"http://www.chengshaojin.com/tags/生活/"}]},{"title":"docker单机环境下的网络与通信","date":"2018-01-22T02:09:14.000Z","path":"2018/01/22/docker单机环境下的网络与通信/","text":"一、docker单机环境的下的网络1.三种原生网络docker提供三种原生网络，在安装的时候就会自动在host创建三个网络，用 docker network ls 命令查看，分别是：bridge、host、none。在创建容器的时候可以通过--network=none/host/bridge 来指定使用哪一种网络。例如： # docker run -it --network=none busybox none网络：什么都没有的网络，这个网络的容器只有一个lo网卡，是一个网络封闭的容器，对一些安全性较高，不需要联网的应用可以使用none网络。host网络：使用host网络的容器与主机的网络配置完全一样，连hostname都一样。host网络的好处是性能好，但是不够灵活。比如端口与host会冲突.bridge网络：安装docker的时候都会默认创建一个 docker0 的linux bridge，如果不指定网络，默认创建的容器的网络都会挂在 docker0 上。bridge网络的容器有一对veth pair,是一对连起来的网卡，一边在容器里，一边挂在host的docker0 上，这样相当于容器里的一头也挂在 docker0 上。用 docker network inspect bridge 看下bridge网络的配置信息，可以看到bridge网络配置的subnet、网关等信息。 2.自定义网络除了上述三种docker自动创建的网络，用户也可以根据业务需求自定义网络。docker提供三种user-defined网络驱动：bridge、overlay、macvlan。overlay与 macvlan用于创建跨主机网络。通过创建bridge驱动来创建bridge网络，ip网段由docker0自动分配，例如： # docker network create --driver bridge my_net1 也可以通过--subnet 和--gateway 自己指定Ip网段： # docker network create --driver bridge --subnet 172.18.22.0/24 --gateway 172.18.22.1 my_net2 使用自定义的网络也是通过--network 指定，容器中的ip 既可以自动分配也可以在启动容器的时候使用 --ip 指定ip。 # docker run -it --network=my_net2 --ip 172.18.22.8 busybox # docker run -it --network=my_net2 centos 注：只有使用--subnet 创建的网络才能指定静态ip。 二、单机环境下容器之间的通信我们可以想到，挂在同一个bridge上的容器网段和网关都相同，肯定是可以互通的，比如使用上述的my_net2网络的两个容器busybox和centos应该是互通的，事实上也是如此。但是分别使用my_net1和my_net2的两个不同网段的容器能通吗？，答案当然是不行！那么怎么才能让他们互通呢？打开路由转发，加一条路由？这样按理来说应该是可以的，但是事实上加了路由，打开路由转发也不通。这就是docker不同网络的隔离性。docker在iptables上drop了两个网桥之间的双向流向。那么到底怎么样两个不同网段的容器才能通信呢？只有加一块网卡了。在一个容器里加一块另一个网桥的网卡，通过这块网卡与另一个容器通信。 容器之间的三种通信方式1.IP通信如上所说，在创建容器时通过--network 指定相应的网络，或者通过 docker network connect 将现有容器加入指定网络，并且都是同一个网络的网卡，这样两个容器就可以通信了。 2.Docker DNS Serverdocker 自带 DNS服务，使容器可以直接通过“容器名”通信。 # docker run -it --network=my_net2 --name=bbox1 busybox # docker run -it --network=my_net2 --name=bbox2 busybox 在bbox1里pingbbox2 可以通 # ping bbox2 注：使用DNS 只能在 user-defined网络中使用，默认的Bridge不能使用。 3.joined 容器joined容器通过容器之间共享网卡和配置信息，通过127.0.0.1来直接通信。 # docker run -d -it --name=web1 httpd # docker run -it --network=container:web1 busybox 这样两个容器内的网卡mac与ip完全一样，busybox可以直接使用127.0.0.1访问web1的httpd服务。 这种通信方式适合以下场景： 1）.不同容器中的应用程序希望高效快速的通信，比如web server与app server。 2）.希望监控其他容器的网络流量，比如运行在独立容器中的网络监控程序。 三、容器与外网的通信容器与外网的通信其实与其他环境与外网的通信一样的，通过 NAT 访问外网，通过 端口映射 使外网访问容器。 1.容器访问外网主机可以访问外网的情况下，使用默认bridge网络的容器默认也能通外网，关键我们理解下通外网的本职：通过 # iptables -t nat -S 可以看到：当使用docker0 这个网桥的bridge网络的容器，向外Ping的时候，docker0收到容器网段的外出包，把它给masquerade处理，而MASQUERADE将包的源地址转换成host的地址发送出去，即做一次网络地址转发。 2.外网访问容器docker 可将容器对外提供服务的端口映射到 host 的某个端口，外网通过该端口访问容器。容器启动时通过-p参数映射端口： # docker run -d -p 80 httpd 容器启动后，可通过 docker ps 或者 docker port 查看到 host 映射的端口。在上面的例子中，httpd 容器的 80 端口被映射到 host 32773 上，这样就可以通过 : 访问容器的 web 服务了。除了映射动态端口，也可在-p 中指定映射到 host 某个特定端口，例如可将 80 端口映射到 host 的 8080 端口：docker run -d -p 8080:80 httpd","categories":[{"name":"技术","slug":"技术","permalink":"http://www.chengshaojin.com/categories/技术/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://www.chengshaojin.com/tags/docker/"}]},{"title":"docker学习笔记（二）","date":"2018-01-20T07:48:57.000Z","path":"2018/01/20/docker基础（二）/","text":"docker底层最重要的两种技术分别是cgroup和namespace,cgroup实现资源限额，namespace实现资源隔离。cgroup可以设置进程使用cpu、内存和io资源的限额，而namespace使得每个容器都觉得自己在独立的使用主机资源，所以实现了容器间资源的隔离。linux的六种namespace对应六种资源：mount、UTS、IPC、PID、Network和User。 而cgroup对资源限额的使用主要体现在以下方面 1.对内存的限额。与操作系统类似，容器使用的内存包括物理内存和swap两种。 -m 或 –memory 设置内存的使用限额 –memory-swap 设置内存+swap的使用限额 如下：123 # docker run -m 200M --memory-swap=300M centos表示运行centos容器，允许该容器使用内存最多200M，swap最多100M。默认情况这两组参数为-1，即不限额。如果启动容器的时候只指定 -m 而不指定 --memory-swap，那么默认 --memory-swap为-m的两倍， 2.对cpu的限额默认情况，所有容器平等使用host的cpu资源没有限制。-c 或 –cpu-share 可以设置容器使用cpu的权重。不指定的话默认值为1024,也就是说 -c 参数并不能为容器设置cpu的绝对使用资源某个容器最终获取到的cpu资源由它占所有容器使用cpu综合的比例决定，如下：12345启动两个容器： # docker run --name \"container1\" -c 1024 centos # docker run --name \"container2\" -c 512 centos表明 container1容器可以得到2倍的container2容器的cpu资源。这只是在cpu资源紧张的时候，如果container1是关闭状态，container2依然可以使用全部的cpu资源 –cpu 可以设置cpu工作线程的数量，最大为host物理cpu个数。可以使用 progrium/stress 镜像来学习如何为容器分配内存和cpu。该镜像也可用于对容器执行压力测试1# docker run --name container1 -it -c 1024 progrium/stress --cpu 1 3.对block IO的限额Block IO 指的是磁盘的读写，docker 可通过设置权重、限制 bps 和 iops 的方式控制容器读写磁盘的带宽。默认情况下，所有容器能平等地读写磁盘，可以通过设置 –blkio-weight 参数来改变容器 block IO 的优先级。–blkio-weight与 –cpu-shares 类似，设置的是相对权重值，默认为 500。限制 bps 和 iops： bps 是 byte per second，每秒读写的数据量。 iops 是 io per second，每秒 IO 的次数。 可通过以下参数控制容器的 bps 和 iops： –device-read-bps，限制读某个设备的 bps。 –device-write-bps，限制写某个设备的 bps。 –device-read-iops，限制读某个设备的 iops。 –device-write-iops，限制写某个设备的 iops。12# docker run -it--device-write-bps /dev/sda:30MB centos 表明限制容器写 /dev/sda 的速率为 30 MB/s，可以通过dd测试。","categories":[{"name":"技术","slug":"技术","permalink":"http://www.chengshaojin.com/categories/技术/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://www.chengshaojin.com/tags/docker/"}]},{"title":"docker学习笔记(一)","date":"2018-01-19T07:41:42.000Z","path":"2018/01/19/docker基础(一)/","text":"最近几个月乱七八糟的学习了很多docker的东西，零散繁杂，不经常用就会遗忘很多。加之，pass层的火热，kubernetes在容器编排方面看似已经无人能敌了，所以整理以下，作为技术储备，哈哈。 安装docker-ce安装基础包12# yum install epel-release -y# yum install axel vim git curl wget lrzsz gcc python-devel yum* python-pip 设置docker官方repo，安装docker CE版本1# yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 系统默认应该没有yum-config-manager,如果没有那就安装它 : yum -y install yum-utils12# yum install -y docker-ce # docker --version 配置docker12345# mkdir /etc/systemd/system/docker.service.d# tee /etc/systemd/system/docker.service.d/docker.conf &lt;&lt; 'EOF'[Service]MountFlags=sharedEOF 编辑 /usr/lib/systemd/system/docker.service12# ExecStart=/usr/bin/dockerdExecStart=/usr/bin/dockerd --insecure-registry 192.168.19.133:4000 启动docker1234# systemctl daemon-reload# systemctl restart docker# systemctl enable docker# docker info 搭建私有仓库12# docker run -d -v /opt/registry:/var/lib/registry -p 5000:5000 \\--restart=always --name registry registry:2 docker容器的几点特别的地方我们都知道docker容器是轻量级的，那么体现在哪些地方呢？1.docker容器与host共用kernel,也就是说docker容器里是没有kernel空间的，只有rootfs，也就是我们常见的/dev、/usr、/bin这类东西，所以容器里是不能升级kernel的。2.镜像的分层结构。绝大部分的docker镜像都是从base镜像中配置安装所需软件而来的，每配置安装一个软件，就在现有的镜像基础上增加一层，新镜像就是从base镜像一层一层叠加而来,而且每一层都是共享的，这样内存中只需存一份base镜像就可以了。当用某个镜像启动一个容器的时候，就会在镜像层之上生成一个容器层，所有对容器的操作都是在容器层，而不会对镜像层有任何改变。所以，镜像可以被多个容器共享。 容器的一些常用操作运行容器1234# docker run -it 镜像 /bin/bash 以交互模式启动并进入一个容器 参数： -d 以后台方式启动容器 --name 给启动的容器命名 进入容器的两种方法12# docker attach 长id# docker exec -it 容器名/长id/短id bash attach 与 exec 主要区别如下: 1.attach 直接进入容器 启动命令 的终端，不会启动新的进程。 2.exec 则是在容器中打开新的终端，并且可以启动新的进程。 如果想直接在终端中查看启动命令的输出，用 attach；其他情况使用 exec。当然，如果只是为了查看启动命令的输出，可以用 docker logs 命令 容器的常用操作123# docker stop/start/restart 容器 # docker pause/unpause 容器 # docker rm 容器","categories":[{"name":"技术","slug":"技术","permalink":"http://www.chengshaojin.com/categories/技术/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://www.chengshaojin.com/tags/docker/"}]},{"title":"kolla部署openstack的pika版","date":"2018-01-18T08:44:28.000Z","path":"2018/01/18/kolla部署openstack的pika版/","text":"用kolla部署openstack真的是超级方便简单，想当年初识openstack的时候我可是用了两周多的时间才搭建起一套没有HA的环境，可今天下午，如果不算下载docker镜像的时间，我只用不到两小时的时间就部署起一套openstack+ceph的高可用融合型环境。kolla本身将openstack包括ceph的所用服务都容器化，所有你要启动某个服务，只需要重启该服务的容器就好了。而且kolla是唯一一个没有任何厂商背景的部署工具，完全开源，已经非常成熟了，我所在公司九州云，已经部署了n多的生产案例了，扩容按理来说应该也是比较方便的。贴上同事整理的部署文档，你只需要仔细一点，按照此方法应该都是可以部署成功的，所以就不多说了。https://www.lijiawang.org/posts/kolla-pike-on-centos.html 关于网络123456789kolla定义网络是在/etc/kolla/global.yml文件中,只需在相关网络的后面写上实际的物理网卡名字就可以了，如下：network_interface: \"eno16777736\" 管理网api_interface: \"&#123;&#123; network_interface &#125;&#125;\"storage_interface: \"&#123;&#123; network_interface &#125;&#125;\" 存储网cluster_interface: \"&#123;&#123; network_interface &#125;&#125;\" 存储集群网tunnel_interface: \"&#123;&#123; network_interface &#125;&#125;\" dns_interface: \"&#123;&#123; network_interface &#125;&#125;\" vxlan网neutron_external_interface: \"eno33554960\" private网网络可以分离，也可以用同一块，但是最少应该两块网卡，因为private网络需要独立。如果你做了网卡绑定的话，就换成bond0或bond1等 关于节点信息12345678910关于节点的定义信息是在multinode文件中，在此文件中你可以用hostname来定义哪些物理机是control,哪些是compute，哪些是storage，哪些是network等等，你可以定义openstack的服务安装在哪些节点上。如下：[control]control01control02control03表示control01、control02、control03是控制节点，control01、control02、control03是三个节点的hostname。[keystone:children]control表示keystone服务安装在所有的control节点 关于要安装哪些组件1234567891011121314这个功能也在global.yml文件中定义，如下：enable_barbican: \"no\"enable_ceilometer: \"yes\"enable_central_logging: \"yes\"enable_ceph: \"yes\"enable_ceph_rgw: \"yes\"enable_chrony: \"yes\"enable_cinder: \"yes\"enable_cinder_backend_hnas_iscsi: \"no\"enable_cinder_backend_hnas_nfs: \"no\"enable_cinder_backend_iscsi: \"no\"enable_cinder_backend_lvm: \"no\"enable_cinder_backend_nfs: \"no\"如果你想安装哪个组件，只需要将该组件行的注释去掉，冒号里写yes即可 当这些都定义完就可以deploy了，你可以在deploy之前先prechecks下，防止一些语法错误。再多说一点，1234567在你已经部署完成的情况下，如果要更改global.yml文件，改完以后执行以下命令：kolla-ansible upgrade -i multinodekolla-ansible post-deploy -i multinode如果修改了/etc/config/[server]/[server].conf文件：kolla-ansible reconfigure -i multinode如果部署失败，可以清除然后重新部署，清除的命令是：kolla-ansible destroy -i multinode --yes-i-really-really-mean-it","categories":[{"name":"技术","slug":"技术","permalink":"http://www.chengshaojin.com/categories/技术/"}],"tags":[{"name":"openstack","slug":"openstack","permalink":"http://www.chengshaojin.com/tags/openstack/"}]},{"title":"Linux字符管理命令","date":"2018-01-09T04:23:18.000Z","path":"2018/01/09/Linux字符管理命令/","text":"平时工作中经常用到grep、awk、sort、sed等字符管理命令，但是自己记性又不好，每次需要去google，索性总结一下，方便以后查询。 cut 截取所需字符-d “n”:定义分界符,即点位-f n:取第几位的字符1234例如：以空格符为分界符,进行第2位截取 cut -d \" \" -f 2 /etc/fstab 以冒号为分界符，进行第1，3位截取 cut -d \":\" -f 1,3 /etc/passwd sed 通过指定的正则表达式完成指定关键字的过滤、截取、修改等操作1.关于替换： 1).sed替换的基本语法为: sed ‘s/原字符串/替换字符串/‘ filename s 表示替换 特殊字符需要使用反斜线“\\”进行转义，单引号是不能用反斜线转义，要用反斜线的话使用双引号。 要处理的字符串包含单引号也用双引号。 注意：在末尾加g替换每一个匹配的关键字，否则只替换每一行的第一个字符串12替换所有匹配关键字 sed 's/原字符串/替换字符串/g' filename 2）三根斜线也可换成别的符号，只要紧跟s定义即可12将分隔符换成问号”?”: sed 's?原字符串?替换字符串?' 注意：sed处理过的输出是直接输出到屏幕上的,使用参数”i”直接在文件中替换 3）多个替换可以在同一条命令中执行,用分号”;”分隔，其格式为:12同时执行两个替换规则 sed 's/^/添加的头部&amp;/g；s/$/&amp;添加的尾部/g' 4）一些特殊字符的使用 ”^”表示行首 ”$”符号如果在引号中表示行尾，但是在引号外却表示末行(最后一行)2.关于删除： 1）d 删除指定行,要在文件内删除，同样加参数i123456删除文件的第1-3行 sed '1,3d' filename 删除文件的第3行到最后一行 sed ‘3,$d’ filename 删除含有指定字段的行 sed '/字符/d' filename 3)多点编辑 使用-e参数12删除第1-3行，替换某字符 sed -e '1,3d' -e 's/原字符/替换的字符/' filename 4)文件操作1234将含有某字段的行写入新的文件中 sed -n '/某字段/w 新文件' filename 将小写改为大写 sed 'y/小写字母/大写字母/' filename awk 通过正则表达式,得到需要的行,列信息123456 查看df -h命令的第2列 df -h | awk '&#123;print $2&#125;' 查看df -h命令的第2,5列 df -h | awk '&#123;print $2,$5&#125;' 列示月份及年份(\\n为换行符) date | awk '&#123;print \"Year:\" $6 \"\\nMonth:\" $2&#125;' sort 默认以排序ASCII方式进行排序[a-z] 参数: -u 去除重复的行 -r 降序排序[z-a] -n 数值排序,默认情况10比2小,主要因为sort判断第一字符的值 -k 以文本的列进行判断 -t 设定分界符123456 对/etc/passwd文件进行升序排序 sort /etc/passwd 对/etc/passwd文件进行降序排序 sort -r /etc/passwd 对/etc/passwd第3列进行数值排序,分界符为: sort -n -k 3 -t : /etc/passwd wc 统计行数、字数、字符数、文件总统计数 参数: -l 统计行数 -c 统计字节数 -w 统计字数(单词数 uniq 检查文本中重复出现的行 -c 显示输出,并在文本行前加出现的次数,但如果 重复行不连续，则不认为是重复的行 -d 只显示重复的行 -u 只显示不重复的行 -f n前N个字段和每个字段前的空白行一起被忽略,字段从0开始编号 -s n 前N个字符被忽略,字符从0开始编号 -w n 对N个字符以后的字符不在检查重复性 tee 读取标准输入的数据，并将其内容输出成文件 说明:指令会从标准输入设备读取数据，将其内容输出到标准输出设备，同时保存成文件 参数: -a:附加到既有文件的后面，而非覆盖它． -i:忽略中断信号。12 查询当前账户并写入who.txt文件中who | tee who.txt","categories":[{"name":"技术","slug":"技术","permalink":"http://www.chengshaojin.com/categories/技术/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://www.chengshaojin.com/tags/Linux/"}]}]